[Operation]
# torch device, falls back to 'cpu' if accel is not available
device=cuda:0
# purge existing workdir, adapters and full output when the program starts
purgeTargetDirectories=false

[Trainer]
# activate trainer
train=true
# save lora adapter to disk after training. Also required for validation and merging
storeAdapter=true
# convert text dataset to chat prompts
trTextDsToChat=true
# if plain texts have been converted to chat prompts, also keep plain texts in dataset (combined)
trTextDsKeepPlainText=true
# path to base model (hf safetensors repo)
locBaseModel=../../lm/models/safetensors/Qwen2.5-1.5B-Instruct/
# path to datasets
locDataset=./testProject/dataset/
# path: working directory
locWorkdir=./testProject/work/
# path: lora adapter output
locAdapter=./testProject/adapter/
# training epochs (how many times each dataset entry is trained)
#   vInplace true: Validation will run after each batch of training epochs until vExpected has been reached, otherwise the next training batch starts
#   vInplace false: Training is executed once for trEpochs epochs
trEpochs=10
# optional int, maximum length of training sequence
trMaxSeqLength=
# optional int, training batch size (reduces memory consumption)
trPerDeviceTrainBatchSize=
# optional bool, determine training batch size (reduces memory consumption)
trFindAutoBatchSize=
# optional int, reduces memory consumption
trGradientAccSteps=
# optional bool, reduces memory consumption
trGradientCheckpointing=
# optional bool, reduces memory consumption
trGroupByLength=true
# optional bool, naively pack traning sequences together (reduces memory consumption)
trPacking=

[Lora]
# As bigger the R the more parameters to train
loraR=32
# A scaling factor that adjusts the magnitude of the weight matrix. It seems that as higher more weight have the new training
loraAlpha=8
# Helps to avoid Overfitting
loraDropout=0.05
# lora_only, all, none
loraBias=none
# lora task type
loraTaskType=CAUSAL_LM

[Validation]
# enable validation
validate=true
# Validate in place during training.
#   In place validation: Validation will run after each batch of training epochs until vExpected has been reached. Requires more memory (both training and grader model are in memory at the same time)
#   Normal validation: Validation is done once after the traning epochs have been finished
vInplace=true
# path: validation dataset
locValidation=./testProject/validation/
# path to validation model (hf safetensors repo)
locGraderModel=../../lm/models/safetensors/Meta-Llama-3.1-8B-Instruct-abliterated/
# expected percentage of questions passed to pass the validation
vExpected=80
# abort processing when validation fails (currently: skip merging to full model). Only relevant for vInplace=false
vAbortOnFail=true
# how many times each question is asked and evaluated, averages out evaluation results
vPasses=3
# maximum response length for validations
vGenMaxTokens=60
# use 4bit quant of the fine tuned model (reduces memory consumption and accuracy), only relevant on gpu. Does not have an effect if vInplace=true, instead of that the training model is used directly
vQuantModel=false
# use 4bit quant of the validation model (reduces memory consumption and accuracy), only relevant on gpu
vQuantGrader=true
# run grader model always on cpu
vGraderOnCpu=false

[Merger]
# merge lora adapter back into the base model
mergeFull=true
# path: merged full output
locFull=./testProject/full/
