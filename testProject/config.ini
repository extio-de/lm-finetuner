[Operation]
# torch device, falls back to 'cpu' if accel is not available
device=cuda:0
# purge existing workdir, adapters and full output when the program starts
purgeTargetDirectories=false
# For debugging
showChatTemplate=true

[Trainer]
# activate trainer
train=true
# save lora adapter to disk after training. Also required for validation and merging
storeAdapter=true
# path to base model (hf safetensors repo)
locBaseModel=../../lm/models/safetensors/Llama-3.2-3B-Instruct-uncensored/
# path to datasets
locDataset=./testProject/dataset/
# path: working directory
locWorkdir=./testProject/work/
# path: lora adapter output
locAdapter=./testProject/adapter/
# path: custom chat template (Jinja template), optional
locCustomPromptTemplate=./testProject/llama3_prompt_min.txt
# training epochs (how many times each dataset entry is trained)
#   vInplace true: Validation will run after each batch of training epochs until vExpected has been reached, otherwise the next training batch starts
#   vInplace false: Training is executed once for trEpochs epochs
#   Optimizer schedule_free_*: Leave trEpochs blank
trEpochs=1
# int, maximum length of training sequence
trMaxSeqLength=2000
# optional scheduler type: linear, constant, cosine, polynomial, inverse_sqrt
trSchedulerType=
# optional optimizer; grokadamw, schedule_free_adamw, schedule_free_sgd
trOptim=
# optional int, training batch size (reduces memory consumption)
trPerDeviceTrainBatchSize=
# optional bool, determine training batch size (reduces memory consumption)
trFindAutoBatchSize=
# optional int, reduces memory consumption
trGradientAccSteps=
# optional bool, reduces memory consumption
trGradientCheckpointing=
# optional bool, reduces memory consumption
trGroupByLength=true
# optional bool, naively pack traning sequences together
trPacking=

[Lora]
# Enable qLora (4bit quantized lora) 
qLora=true
# As bigger the R the more parameters to train
loraR=32
# A scaling factor that adjusts the magnitude of the weight matrix. It seems that as higher more weight have the new training
loraAlpha=8
# Helps to avoid Overfitting
loraDropout=0.05
# lora_only, all, none
loraBias=none
# lora task type
loraTaskType=CAUSAL_LM
# layers to train (all linear layers are trainable)
loraLayers=q_proj,k_proj

[Validation]
# enable validation
validate=false
# Validate in place during training.
#   In place validation: Validation will run after each batch of training epochs until vExpected has been reached. Requires more memory (both training and grader model are in memory at the same time)
#   Normal validation: Validation is done once after the traning epochs have been finished
vInplace=true
# path: validation dataset
locValidation=./testProject/validation/
# path to validation model (hf safetensors repo)
locGraderModel=../../lm/models/safetensors/Meta-Llama-3.1-8B-Instruct-abliterated/
# expected percentage of questions passed to pass the validation
vExpected=80
# abort processing when validation fails (currently: skip merging to full model). Only relevant for vInplace=false
vAbortOnFail=true
# how many times each question is asked and evaluated, averages out evaluation results
vPasses=3
# maximum response length for validations
vGenMaxTokens=60
# use 4bit quant of the fine tuned model (reduces memory consumption and accuracy), only relevant on gpu. Does not have an effect if vInplace=true, instead of that the training model is used directly
vQuantModel=false
# use 4bit quant of the validation model (reduces memory consumption and accuracy), only relevant on gpu
vQuantGrader=true
# run grader model always on cpu
vGraderOnCpu=false

[Merger]
# merge lora adapter back into the base model
mergeFull=true
# path: merged full output
locFull=./testProject/full/
